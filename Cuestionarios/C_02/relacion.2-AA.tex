\subsection*{MATRICES Y OPTIMIZACIÓN}
\label{mat&op}
\begin{enumerate}

\item (B) Verificar las siguientes propiedades de las matrices
\begin{enumerate}
    \item Dada una matriz $\rmX$ de números reales las matrices $\mathrm{XX^T}$ y  $\mathrm{X^TX}$ son simétricas.
    \item Sea $\rmX$ una matriz de números reales, Sea $\mathrm{X=UDV^T}$ su descomposición en valores singulares (SVD). Calcular la SVD de $\mathrm{X^TX}$ y $\mathrm{XX^T}$.
    \item Establezca alguna relación entre los valores singulares de las dos descomposiciones encontradas en el apartado anterior y los valores singulares de $\rmX$.
    \item Probar que si una matriz cuadrada $\rmX$ tiene inversa, entonces $\mathrm{\rmX^{-1}=VD^{-1}U^T}$ si $\mathrm{SVD(X)=UDV^T}$ ¿Cómo sería si además es simétrica?
    \item Sean $\bfx$ e $\bfy$ dos vectores de valores observados. Escriba de forma matricial la expresión
    \[
    cov(\bfx,\bfy)=\frac{1}{N}\sum_{i=1}^N (x_i-\bar{\bfx})(y_i-\bar{\bfy})
    \]
    donde $\bar{\bfx}$ y $\bar{\bfy}$ representan la media de los vectores $\bfx$ e $\bfy$ respectivamente.
    \item Considere ahora una matriz $\rmX$ cuyas columnas representan vectores de observaciones. La matriz de covarianzas asociada a la matriz $\rmX$ es el conjunto de covarianzas definidas por cada dos de sus vectores columnas. Defina la expresión matricial que define la matriz $cov(\rmX)$ en función de la matriz $\rmX$
\end{enumerate}
%
    \item (B)Considerar la matriz hat $\mathrm{H}=\mathrm{X(X^TX)^{-1}X^T}$, donde $\rmX$ es una matriz  $N\times (d+1)$, y $\mathrm{X^TX}$ es invertible.
    \begin{enumerate}
        \item Mostrar que H es simétrica 
        \item Mostrar que $\mathrm{H^K=H}$ para cualquier entero K ( Ayuda: probar $\mathrm{H^2=H}$)
        \item \label{b2} Si $\rmI$ es la matriz identidad de tamaño N, mostrar que  $\mathrm{(I-H)^K=I-H}$ para cualquier entero positivo K
        \item \label{b3}Mostrar que $\mathrm{traza(H)}=d+1$, donde la traza significa la suma de los elementos de la diagonal principal. (Ayuda: usar SVD(X))
    \end{enumerate}
    %
    %
    %
    \item (Multiplicadores de Lagrange)  Lagrange propuso una técnica para resolver el siguiente problema de optimización 
    \[
    \begin{array}{c}
    \mmax_{x,y} g(x,y) \\
    \hbox{Sujeto a } f(x,y)=0
    \end{array}
    \]
    Es decir, buscar el máximo de la función $g$ en un recinto del plano $x-y$ definido por los valores nulos de la función $f$. La solución es transformar este problema de optimización con restricciones en un problema de optimizaciíon sin restricciones y resolver este último derivando e igualando a cero. Para ello construye una nueva función denominada Lagrangiana que se define como
    \[
    \mathcal{L}(x,y,\lambda)=g(x,y)-\lambda f(x,y)
    \]
    siendo $lambda$ una constante y prueba que la solución de óptimo de $\mathcal{L}$ es la misma que la del problema inicial. Por ello para obtener dicha solución solo hay que calcular la solución del sistema de ecuaciones dado por $\nabla_{x,y,\lambda}\mathcal{L}(x,y,\lambda)=0$. En el caso de que exista más de una restricción en igualdad cada una de ellas se añade a la Lagrangiana de la misma manera pero con un $\lambda$ diferente.
    \[
    \mathcal{L}(x,y,\lambda_1,\cdots,\lambda_n)=g(x,y)-\sum_{i=1}^n\lambda_i f_i(x,y)
    \]
    Resolver los siguientes problemas: 
    \begin{enumerate}
        \item Encontrar el punto $(x_0,y_0)$ sobre la línea $ax+by+d=0$ que este más cerca del punto $(x_1,y_1)$.
        \item Encontrar los valores extremos de $f(x,y)=2x+y$ sujeto a la restricción $x^2+y^2=4$.
        \item La distancia entre dos curvas en el plano está dada por el mínimo de la expresión $\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ donde $(x_1,y_1)$ está sobre una de las curvas y $(x_2,y_2)$ está sobre la otra. Calcular la distancia entre la línea $x+y=4$ y la elipse $x^2+2y^2=1$.
    \end{enumerate}
    
    En el caso de que algunas de las condiciones de restricción estén definidas en términos de desigualdad (<, $\leq$, etc), entonces las condiciones para que la solución del problema sin restricción coincida con la solución del problema con restricciones  cambian respecto del caso Lagrangiano, dichas condiciones se denominan las condiciones de \rm{Karush-Kuhn-Tucker}.
    \item La programación líneal es una técnica de optimización que busca el óptimo ( máximo o mínimo) de una función lineal en una región de valores definida por un sistema de ecuaciones en desigualdad. En concreto, 
    \[
    \begin{array}{c}
    \mMin_{\bfx} \mathrm{\mathbf{c}^T\bfx} \\
    \hbox{Sujeto a } \mathrm{A\bfx \leq \mathbf{b}}
    \end{array}
    \]
    donde \textbf{c} y \textbf{b} son vectores y A es una matriz.
    
    \begin{enumerate}
    \item (B) Para un conjunto de datos linealmente separable mostrar que para algún $\bfw$ se debe de verificar la condición  $\mathrm{y_n\bfw^T\bfx_n>0 }$ para todo $\mathrm{(\bfx_n,y_n)}$ del conjunto.
    \item (B) Formular un problema de programación lineal que resuelva el problema de la búsqueda del hiperplano separador. Es decir, identifique quienes son  A, \textbf{z}, \textbf{b} y \textbf{c} para este caso.
    \item (M) Si los datos no son separables, la condición del primer apartado no se verifica para todos los puntos. Por tanto introducimos la holgura $\xi_n>0$ para capturar la cantidad de violación para el ejemplo $\bfx_n$. Por tanto, para $n=1,\dots,N$, tenemos
    
    \centerline{
    \begin{tabular}{c}
         $y_n(\bfw^T\bfx_n \geq 1-\xi_n$ \\
         $\xi_n \geq 0$
    \end{tabular}}
    
    Logicamente nos gustaría minimizar la cantidad de violación. Una aproximación intuitiva es minimizar $\sum_{n=1}^N \xi_n$, i.e deseamos encontrar $\bfw$ que resuelve
    
    \centerline{
    \begin{tabular}{rl}
        $\mmin_{\bfw,\xi_n}$ & $\sum_{n=1}^N \xi_n$\\
        \hbox{ sujeto a } & $y_n(\bfw^T\bfx_n)\geq 1-\xi_n$\\
        \multicolumn{2}{c} {$\xi_n \geq 0$}
    \end{tabular}}
    
    donde las desigualdades deben de verificarse para $n=1,\dots,N$. Formular este problema como un problema de programción lineal
    \item (M) Argumentar que el problema de programación lineal que encontró en el apartado anterior  y el problema de optimización del ejercicio.\ref{P3.5} de la sección MINIMIZACIÓN ITERATIVA son equivalentes.
    \end{enumerate}
    %
    \end{enumerate}
    %
    %
    %
    \subsection*{REGRESIÓN}
    \label{reg}
    \begin{enumerate}
    
    \item (M) Probar que en el caso general de funciones con ruido se verifica que $\bbE_{\calD}[\Eout]= \sigma^2+\texttt{\textbf{bias}}+\texttt{\textbf{var}}$ ( ver transparencias de clase)
    
    \item\label{3.4} (A) Consideremos que los datos están generados por una función con ruido $y=\bfw^{*T}\bfx+\epsilon$, donde $\epsilon$ es un término de ruido con media cero y varianza $\sigma^2$, que está generado de forma independiente para cada muestra $(\bfx, y)$. Por tanto el error esperado del mejor ajuste posible a esta función es $\sigma^2$.
    
    Supongamos una muestra de datos $\Sample$ donde  el ruido en cada $y_n$ se nota como $\epsilon_n$ y sea ${\mathbf{ \epsilon}}=[\epsilon_1,\epsilon_2,\dots,\epsilon_N]^T$; asumimos que $\mathrm{X^TX}$ es invertible. Seguir los pasos que se muestran a continuación y mostrar que el error esperado (i.e. error esperado de entrenamiento) de regresión lineal respecto de $\calD$  está dado por
    \[
    \bbED [\Ein(\bfw_{\mathrm{lin}})]=\sigma^2\left(1-\frac{d+1}{N}\right)
    \]
    \begin{enumerate}
        \item Mostrar que la estimación de $\mathrm{\mathbf{y}}$ está dada por $\mathrm{\mathbf{\hat y} = X\bfw^*+ H \mathbf{\epsilon} }$
        \item \label{b1} Mostrar que el vector de errores dentro de la muestra $\mathrm{\mathbf{\hat{y}-y}}$ puede expresarse como el producto de una matriz por $\mathbf{\epsilon}$ ¿Cual es la matriz?
        
        \item Expresar $\Ein(\bfw_{\mathrm{lin}})$ en función de $\mathbf{\epsilon}$  usando el apartado \ref{b1} y simplificar la expresión usando el ejercicio \ref{b2}.
        
        \item Probar que $\bbED[\Ein(\bfw_{\mathrm{lin}})]=\sigma^2\left(1-\frac{d+1}{N}\right)$ usando el apartado anterior y la independencia de los errores, $\epsilon_1,\epsilon_2,\dots,\epsilon_N$. (Ayuda: Tener en cuenta la suma de los elementos de la diagonal de una matriz. Además el apartado \ref{b3} también es relevante)
        
        \item Para analizar el error esperado fuera de la muestra, vamos a considerar un caso que es fácil de analizar. Consideremos un conjunto de test $\calD_{\mathrm{test}}=\{ (\bfx_1,y_1^{'}),\dots,(\bfx_N,y_N^{'}) \}$  que comparte los mismas entradas que $\calD$ pero con  términos de ruido de valor diferente. Notemos el ruido en $y_n^{'}$ como $\epsilon_n^{'}$ y sea  $\mathbf{\epsilon^{'}}=[\epsilon_1^{'},\epsilon_2^{'},\dots,\epsilon_N^{'}]^T$. Definir $E_{\mathrm{test}}(\bfw_{\mathrm{lin}}})$ como el error cuadrático medio sobre $\calD_{\mathrm{test}}$.
        \begin{enumerate}
            \item Probar que $\bbEDeps{\mathbf{\epsilon^{'}}}[E_{\mathrm{ test}}(\bfw_{\mathrm{lin}}})]=\sigma^2\left(1-\frac{d+1}{N}\right)$
            
            Este error de test especial, $E_{\mathrm{test}}$, es un caso muy restrictivo del caso general de error-fuera-de-la-muestra.
        \end{enumerate}
    \end{enumerate}
    %
    %
    %
    \item  (B) Consideremos las mismas condiciones generales del enunciado del ejercicio anterior.
    Considerar ahora $\sigma=0.1$ y $d=8$, ¿cual es el más pequeño tamaño muestral que resultará en un valor esperado de $\Ein$ mayor de $0.008$?.
    %
    %
    %
    
\item (M) En regresión lineal, el \texttt{error fuera de la muestra} esta dado por
\[
\Eout(h)=\bbE[(h(\bfx)-y)^2]
\]
Mostrar que entre todas las posibles hipótesis, la que minimiza $\Eout$ está dada por
\[
h^*(\bfx)=\bbE[y|\bfx]
\]
La función $h^*$ puede considerarse una función  determinista, en cuyo caso podemos escribir $y=h^*(\bfx)+\epsilon(\bfx)$, donde $\epsilon(\bfx)$ es una varible de ruido independiente. Mostrar que $\epsilon(\bfx)$ tiene valor esperado $0$.
%
%
%
\item (B) Suponiendo que $\rmX^T\rmX$ es invertible, mostrar por comparación directa con la ecuación
\[
\Ein(\bfw)=\frac{1}{N}(\bfw^T\rmX^T\rmX\bfw-2\bfw^T\rmX^T\bfy+\bfy^T\bfy)
\]
que $\Ein(\bfw)$ puede escribirse como
\[
\Ein(\bfw)=(\bfw-\rmXXi\rmX^T\bfy)^T(\rmX^T\rmX)(\bfw-\rmXXi\rmX^T\bfy)+\bfy^T(\rmI-\rmX\rmXXi\rmX^T)\bfy
\]
%

%
\item (A)
Consideremos las condiciones del problema de regresión lineal establecidas en el ejercicio.\ref{3.4} de esta sección, donde los datos se generan a partir de una verdadera relación lineal más un ruido. El rudio se supone i.i.d. con media cero y varianza $\sigma^2$. Supongamos que la matriz de segundos momentos $\Sigma=\bbE_{\mathrm{ \bfx}}[\mathrm{\bfx\bfx^T}]$ es non singular. Seguir los pasos que se señalan para verificar que con alta probabilidad, el \texttt{error-fuera-de-la-muestra} en promedio vale
\[
\Eout(\bfw_{\mathrm{lin}})=\sigma^2\left(1+\frac{d+1}{N}+o(\frac{1}{N})\right)
\]
\begin{enumerate}
    \item para un punto de test $\bfx$ mostrar que el error $y-g(\bfx)$ es
    \[
    \epsilon-\bfx^T\rmXXi\rmX^T\mathbf{\epsilon}
    \]
    donde $\epsilon$ es el valor del ruiso para el punto de test y $\mathbf{\epsilon}$ es el vector de valores de ruido en los datos.
    \item Calcular la esperanza  respecto del punto de test, i.e. $\bfx$ y $\epsilon$, para obtener una expresión para $\Eout$. Mostrar que
    \[
    \Eout=\sigma^2+\mathrm{traza( \Sigma (X^TX)^{-1} X^T\mathbf{\epsilon\epsilon^T}X^T(X^TX)^{-1}}
    \]
    (ayuda: traza(AB)=traza(BA) y esperanza y traza conmutan)
    \item Que es $\bbE_{\mathrm{\mathbf{\epsilon}}}[\mathrm{\mathbf{\epsilon\epsilon^T}}]$ ?
    \item Calcular la esperanza respecto a $\mathrm{\mathbf{\epsilon}}$ para ver que en promedio
    \[
    \Eout=\sigma^2+\frac{\sigma^2}{\mathrm{N}}\mathrm{traza(\Sigma(\frac{1}{N}X^TX)^{-1}})
    \]
    Note que $\mrm{\frac{1}{N}X^TX=\frac{1}{N}\sum_{n=1}^N \bfx_n\bfx_n^T}$ es una estimación de $\Sigma$ con N muestras. 
    
    Por tanto $\mrm{\frac{1}{N}X^TX\approx\Sigma}$. Si  $\mrm{\frac{1}{N}X^TX = \Sigma}$ ¿cuanto vale $\Eout$ en promedio ?.
    \item Mostrar que ( después de calcular la esperanza respecto de todos los datos muestrales) con alta probabilidad
    \[
    \Eout=\sigma^2\left(1+\frac{d+1}{N}+o(\frac{1}{N})\right)
    \]
    (Ayuda: por la ley de los grandes números $\mrm{\frac{1}{N}X^TX}$ converge en probabilidad a $\Sigma$, y por continuidad de la inversa de $\Sigma$, $\mrm{(\frac{1}{N}X^TX)^{-1}}$ converge en probabilidad a $\Sigma^{-1}$.
    %
   
    \end{enumerate}
    
    \item (B) Los polinómios de Legrendre son una familia de polinomios ortogonales que son muy útiles en regresión ya que permiten definir una clase de funciones ortogonales. Los dos primeros polinomios están dados por $L_0(x)=1$ y $L_1(x)=x$ respectivamente. Los polinomios de orden superior están definidos por la recurrencia
    \[
    L_k(x)=\frac{2k-1}{k}xL_{k-1}(x)-\frac{k-1}{k}L_{k-2}(x)
    \]
    \begin{enumerate}
        \item Usar la recursión para desarrollar un algoritmo eficiente para calcular $L_0(x),\dots,L_K(x)$ dado $x$. El algoritmo deberá ejecutarse en tiempo lineal en $K$.
        \item Dibujar los seis primeros polinomios de Legendre en unos mismos ejes en el rango $x\in[-1,1]$
        \item Ver que cada $L_k(x)$ es una combinación lineal de monomios $x^k, x^{k-2},\dots$ y que se cumple
        \[
        L_k(-x)=(-1)^kL_k(x)
        \]
        \item NOTA: La propiedad de ortogonalidad significa que  
        \[
        \int_{-1}^1 L_k(x)L_l(x) dx =\left\{
        \begin{tabular}{lc}
        $0$ & $l\neq k$ \\
        $\frac{2}{2k+1}$ & $l=k$
        \end{tabular}
        \right .
        \]
    \end{enumerate}
    \end{enumerate}
    %
    %
    %
    \subsection*{REGRESION LOGISTICA}
    \label{RL}
    \begin{enumerate}
    \item (B) Considerar la función 
    \[
    \tanh(s)=\frac{e^s-e^{-s}}{e^s+e^{-s}}
    \]
    \begin{enumerate}
        \item  ¿Como está relacionada esta función con la función logística $\sigma(s)$?
        \item Mostrar que $\tanh(s)$ converge a un valor asintótico finito para valores de $|s|$ grandes y no converge a ningún valor para valores de $|s|$ pequeños.
        \item Dibujar la función y compararla con la función $g(s)=-1$ para $s<0$ y $g(s)=+1$ para $s\geq 0$.
    \end{enumerate}
%    
\item (M) Supongamos que queremos predecir una función objetivo con error (i.e. estocástica)  $P(y|\bfx)$ a partir de muestras etiquetadas con valores $\pm 1$ y de funciones hipótesis que notamos por $h$ 
\begin{enumerate}
%   
\item Escribir la función de Máxima Verosimilitud de una muestra de tamaño N
\item\label{cross1} Mostrar que  la estimación de Máxima Verosimilitud se reduce a la tarea de encontrar la función $h$ que minimiza
\[
\Ein(\bfw)=\sum_{n=1}^N \llbracket  y_n=+1 \rrbracket \ln{\frac{1}{h(\bfx_n)}}+ \llbracket y_n=-1 \rrbracket \ln{\frac{1}{1-h(\bfx_n)}}
\]
%
\item Para el caso $h(x)=\sigma(\bfw^T\bfx)$ mostrar que minimizar el error de la muestra en el apartado anterior es equivalente a minimizar el error muestral 
\[
\Ein(\bfw)=\frac{1}{N}\sum_{n=1}^{N} \ln{\left(1+e^{-y_n\bfw^T\bfx_n}\right)}
\]
\end{enumerate}
Nota: dadas dos distribuciones de probabilidad $\{p,1-p\}$ y $\{q,1-q\}$ de variables aleatorias binarias, la entropía cruzada para estas distribuciones se define en teoría de la información por la expresión
\[ p\log{\frac{1}{q}}+(1-p)\log{\frac{1}{1-q}}\]
El error de la muestra en el apartado.\ref{cross1} corresponde a una medida de error de entropía cruzada de los datos $(\bfx_n,y_n)$ con $p=\llbracket y_n=+1 \rrbracket$ y $q=h(\bfx_n)$.
%

 \item Consideremos el caso de la verificación de la huella digital (ver transparencias de clase). Tras aprender con un modelo de regresión logística a partir de datos obtenemos una función una hipótesis final
    \[
    g(x)=\bbP[y=+1|\bfx]
    \]
    que representa la estimación de la probabilidad de que $y=+1$. Suponga que la matriz de coste está dada por 
    \vspace{10pt}
    
    \centerline{
    \begin{tabular}{c r| c c}
         &  &  \multicolumn{2}{c} {Verdadera Clasificación}\\
         &  &  +1 (persona correcta) & -1 (intruso)\\\hline
         decisión  & {+1} & 0 & $c_a$ \\
         decisión & {-1} & $c_r$ & 0 \\\\
    \end{tabular}}
Para una nueva persona con huella digital $\bfx$, calculamos $g(\bfx)$ y tenemos que decidir si aceptar o rechazar a la persona ( i.e. tenemos que usar una decisión 1/0). Por tanto aceptaremos si $g(\bfx)\geq \kappa$, donde $\kappa$ es un umbral.
\begin{enumerate}
    \item Definir la función de costo(aceptar) como el costo esperado si se acepta la persona. Definir de  forma similar el costo(rechazo). Mostrar que
     \vspace{10pt}
     
    \centerline{
    \begin{tabular}{rcc}
    costo(acceptar) & = & $(1-g(\bfx))c_a$\\
     costo(rechazar) & = & $g(\bfx)c_r$ \\
    \end{tabular}}
    \item Usar el apartado anterior para derivar una condición sobre $g(x)$ para aceptar la persona y mostrar que
    \[
    \kappa=\frac{c_a}{c_a+c_r}
    \]
    \item Usar las matrices de costo para la aplicación del supermercado y la CIA (transparencias de clase) para calcular el umbral $\kappa$ para cada una de las dos clases. Dar alguna interpretación del umbral obtenido.
\end{enumerate}

%
\item (B) Considerar la expresión de $\Ein$ dada en \ref{cross1} de esta sección. Nuestro interés es derivar la expresión de la regla de adaptación de parámetros que genera el uso del método de optimización de Newton. La expresión general de la regla de adaptación de Newton es:
\[
\theta^{(t+1)}=\theta^{(t)}-\mathrm{H^{-1}}\nabla_\theta(\Ein)
\]
Para ello   
\begin{enumerate}
    \item Calcular la expresión analítica en forma vectorial del gradiente $\nabla_\theta(\Ein)$
    \item Calcular la expresión analítica en forma vectorial para la matriz Hessiana $\rmH$ (recordar que la matriz Hessiana es la matriz de las derivadas segundas)
\end{enumerate}
%
%
%
\item (B) En regresión logística mostrar que
\[
\nabla\Ein(\bfw)=-\frac{1}{N}\sum_{n=1}^{N}\frac{y_n\bfx_n}{1+e^{y_n\bfw^T\bfx_n}}= \frac{1}{N}\sum_{n=1}^{N}-y_n\bfx_n\sigma(-y_n\bfw^T\bfx_n)
\]
 
Argumentar que un ejemplo mal clasificado contribuye  al gradiente más que un ejemplo bien clasificado.
%
%
%
\item (M) Considerar las siguientes medidas puntuales de error, $\mathbf{\mrm{eclass}}(s,y)=\llbracket y\neq \signo{s}\rrbracket$, $\mathbf{\mrm{esq}}(s,y)=(y-s)^2$, y $\mathbf{\mrm{elog}}(s,y)=\ln{(1+\exp{(-ys)})}$, donde la señal  $s=\bfw^T\bfx$.
\begin{enumerate}
    \item Para $y=+1$, dibujar $\mathbf{\mrm{eclass}},\mathbf{\mrm{sq}}$ y $\frac{1}{\ln{2}}\mathbf{\mrm{elog}}$ versus $s$ en los mismos ejes.
    \item Mostrar que $\mathbf{\mrm{eclass}}(s,y)\leq\mathbf{\mrm{esq}}(s,y)$, y por tanto que el error de clasificación esta acotado superiormente por el error cuadrático.
    \item Mostrar que $\mathbf{\mrm{eclass}}(s,y) \leq \frac{1}{\ln{2}}\mathbf{\mrm{elog}}(s,y)$, y como en el apartado anterior obtener una cota superior (salvo una constante) usando el error de regresión logística.
\end{enumerate}

Nota: Estas cotas indican que minimizar el error cuadrático o el error de regresión logística deberían hacer decrecer también el error de clasificación, lo que justifica  el uso de los pesos devueltos por regresión lineal o regresión logística como aproximaciones para la clasificación.
%
%
%
\item (M) 
\begin{enumerate}
    \item Definamos el error en un punto $(\bfx_n,y_n)$ por
    \[
    \bfe{n}(\bfw)=\mmax(0,-y_n\bfw^T\bfx_n)
    \] 
    Argumentar que el algoritmo PLA puede interpretarse como SGD sobre $\bfe{n}$ con tasa de aprendizaje $\nu=1$.
    \item Para regresión logística con vector de pesos $\bfw$ muy grande, que minimizar $\Ein$ usando SGD es similar a PLA. ( Otra indicación de que los pesos de regresión logística pueden ser usados como buena aproximación en clasificación)
\end{enumerate}
%
%
%
\iffalse
\item Justificar la aseveración de que $\hat{\bfv}=\frac{\nabla\Ein(\bfw(0))}{||\nabla\Ein(\bfw(0))||}$ es la dirección de máximo decremento de $\Ein$ solo si el valor de $\eta$ es pequeño.

(Ayuda:
$
\Ein(\bfw(0)+\eta\mathbf{\hat{v}})-\Ein(\bfw(0))
=\eta\nabla\Ein(\bfw(0))^T \mathbf{\hat{v}}+O(\eta^2)
$)
\fi
%
%
%
\end{enumerate}
%
%
\subsection*{TRANSFORMACIONES NO LINEALES}
\begin{enumerate}
\item (B) \label{tnl} Consideremos la siguiente transformación de características $\Phi_1(\bfx)=(1,x_1^2,x_2^2)$ ¿Que clase de curva genera en $\calX$ los hiperplanos $\hbfw$ de $\calZ$ correspondientes a los siguientes casos
\begin{enumerate}
    \item $\tilde{w}_1 >0$, $\tilde{w}_2<0$
    \item $\tilde{w}_1 >0$, $\tilde{w}_2=0$
    \item $\tilde{w}_1 >0$, $\tilde{w}_2>0$, $\tilde{w}_0\leq0$
    \item $\tilde{w}_1 >0$, $\tilde{w}_2>0$, $\tilde{w}_0>0$
\end{enumerate}
%
%
%
\item (B) \label{gft1}Sabemos que en el plano Euclídeo, el modelo perceptron $\calH$ no puede implementar las 16 dicotomías sobre 4 puntos, es decir, $\gf(4)<16$. Considere la transformación usada en el ejercicio.\ref{tnl} de esta sección. Mostrar que
\begin{enumerate}
    \item ${\gf}_{_{\Phi}}(3)=8$
    \item ${\gf}_{_{\Phi}}(4)<16$
    \item $m_{{\calH}\cup {\calH}_{\Phi}}(4)=16$
\end{enumerate}
Esto significa que si usamos líneas $\dvc=3$, si usamos elipses $\dvc=3$, si usamos lineas y elipses $\dvc>3$
%
%
%
\item (B) \label{fi2}Considerar ahora la transformación $\Phi_2(\bfx)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$. ¿Que hiperplanos $\hbfw$ de $\calZ$ representan a las siguientes curvas frontera de $\calX$ ?
\begin{enumerate}
    \item La parábola $(x_1-3)^2+x_2=1$
    \item El círculo $(x_1-3)^2+(x_2-4)^2=1$
    \item La elipse $2(x_1-3)^2+(x_2-4)^2=1$
    \item La hipérbola $(x-1-3)^2-(x_2-4)^2=1$
    \item La elipse  $ 2(x_1+x_2-3)^2+(x_1-x_2-4)^2=1$
    \item La línea $ 2x_1+x_2=1$
\end{enumerate}
%
\item (M) Considerar la transformación $\Phi_2$ del ejecicio.\ref{fi2} de esta sección como $\Phi$.
\begin{enumerate}
    \item Mostrar que $\mathrm{d_{vc}(\calH_\Phi)}\leq 6$
    \item Mostrar que $\mathrm{d_{vc}(\calH_\Phi)} > 4$ (Ayuda: Ejercicio.\ref{gft1} de esta sección)
    \item Dar una cota superior de $\mathrm{d_{vc}(\calH_{\Phi_k})}$ para $\calX=\bbR^2$
    \item Definir una nueva transformación
    \[
    \tilde{\Phi}_2:\bfx\rightarrow ( 1,x_1,x_2,x_1+x_2,x_1-x_2,x_1^2,x_1x_2,x_2x_1,x_2^2), \mbox{ para } \bfx\in\bbR^2
    \]
    Argumentar que $\mathrm{d_{vc}(\calH_{\Phi_2})}=\mathrm{d_{vc}(\calH_{\tilde{\Phi}_2}})$. Es decir, mientras que $\tilde{\Phi}_2(\calX)\in\bbR^9$, $\mathrm{d_{vc}(\calH_{\tilde{\Phi}_2}})\leq 6 <9$. Por tanto la dimensión de $\Phi(\calX)$ solo da una cota superior de $\mathrm{d_{vc}(\calH_\Phi)}$, y el valor exacto de $\mathrm{d_{vc}(\calH_\Phi)}$ puede depender de las componenetes de la transformación.
\end{enumerate}



\item (B) Considerar la transformación polinomial de Q-ésimo orden, $\Phi_Q$ para $\calX=\bbR^d$. ¿Cuál es la dimensión del espacio de características $\calZ$ ( excluyendo la coordenada fija $z_0=1$)? Evaluar su resultado para $d\in\{2,3,5,10\}$ y $Q\in\{2,3,5,10\}$
%


\item (A) Transformar las características a espacios de alta dimensionalidad no es lo único que podemos hacer. También podemos buscar un buen compromiso disminuyendo el número de variables originales. Es decir buscar espacios de menor dimensionalidad en los que potencialmente tengamos menor error de generalización. Consideremos la siguiente transformación de características, que transforma un $\bfx$ d-dimensional en un $\bfz$ unidimensional
\[
\Phi_k(\bfx)=(1,x_k)
\]
Sea $\calH_k$ el conjunto de perceptrones en el nuevo espacio de características.
\begin{itemize}
    \item Probar que $\dvc(\calH_k)=2$
    \item Probar que $\dvc(\cup_{k=1}^d\calH_k)\leq 2(\log_2 d +1)$
\end{itemize}
Nota: $\calH_k$ constituye el conjunto de de decisiones "stump" para la dimensión $k$ 
%


\item (M) Escribir los pasos del algoritmo que combina $\Phi_3$ con regresión lineal. ¿Que pasa si usamos $\Phi_{10}$?. ¿Donde aparece el principal cuello de botella de cómputo del algoritmo?
\end{enumerate}

\subsection*{MINIMIZACIÓN ITERATIVA: SGD}
\label{minIter}
\begin{enumerate}

\item (M) Una modificación del algoritmo perceptron denominada ADALINE, incorpora en la regla de adaptación una poderación sobre la cantidad de movimiento necesaria. En PLA se aplica $\bfw_{new}=\bfw_{old}+y_n\bfx_n$ y en ADALINE se aplica la regla $\bfw_{new}=\bfw_{old}+\eta(y_n-\bfw^T\bfx_n)\cdot\bfx_n$. Ahora vamos a interpretar ADALINE como un proceso de optimización. Considerar $\Ein(\bfw))=(\mmax(0,1-\ywx))^2$. 
\begin{enumerate}
    \item Mostrar que $E_n(\bfw)$ es continua y diferenciable. Escribir el gradiente $\nabla E_n(\bfw)$.
    \item Mostrar que $E_n(\bfw)$ es una cota superior para $\llbracket\signo{\wx}\neq y_n\rrbracket$. Por tanto $\frac{1}{N}\sum_{n=1}^N E_n(\bfw)$ es una cota superior para  $\Ein(\bfw)$, el error de entrenamiento de clasificación.
    \item Argumentar que el algoritmo ADALINE con la regla mencionada es equivalente a gradiente descendente estocástico ( SGD) sobre $\frac{1}{N}\sum_{n=1}^N E_n(\bfw)$
\end{enumerate}
%
\item (B) Si queremos implementar el algoritmo PLA  usando SGD. ¿Que función de perdida deberiamos de usar?
%%
\item (M) \label{P3.5} Considerar $\Ein(\bfw)=\mmax(0,1-\ywx)$. 
\begin{enumerate}
    \item Mostrar que $E_n(\bfw)$ es continua y diferenciable excepto cuando $y_n=\wx$. 
    \item Mostrar que $E_n(\bfw)$ es una cota superior para $\llbracket\signo{\wx}\neq y_n\rrbracket$. Por tanto $\frac{1}{N}\sum_{n=1}^N E_n(\bfw)$ es una cota superior para  $\Ein(\bfw)$, el error de entrenamiento de clasificación.
    \item Aplicar gradiente descendente estocástico (SGD) sobre $\frac{1}{N}\sum_{n=1}^N E_n(\bfw)$ (ignorando la singularidad de $y_n=\wx$) y derivar un nuevo algoritmo perceptron (i.e. nueva regla de actualización de pesos)
\end{enumerate}
%
\end{enumerate}
%
\subsection*{SOBREAJUSTE}
\label{overfit}

\begin{enumerate}
    \item (B) El ruido determinista depende de $\calH$, ya que algunos modelos aproximan mejor a $f$ que otros.
    \begin{enumerate}
        \item Suponer que $\calH$ es fija y que incrementamos la complejidad de $f$. 
        \item Suponer que $ f$ es fija y decrementamos la complejidad de $\calH$
    \end{enumerate}
    Contestar para ambos escenarios: ¿En general subirá o bajará el ruido determinista? ¿La tendencia a sobrejaustar será mayor o menor? (Ayuda: analizar los detalles que influencian el sobreajuste)
    %
    \item (B) Si $\lambda<0$ en el error aumentado $\Eaug(\bfw)=\Ein(\bfw)+\lambda\bfwt\bfw$, ¿a que orden de restricción ``suave'' corresponde? ( Ayuda: analizar que tipos de pesos favorece $\lambda <0$).
    
\end{enumerate}

\subsection*{REGULARIZACIÓN Y SELECCIÓN DE MODELOS}
\label{modelsel}

\begin{enumerate}
    \item (B) La técnica de regularización de Tikhonov es bastante general al usar la condición
    \[
    \bfwt\mathrm{\Gamma^T\Gamma}\bfw\leq C
    \]
    que define relaciones entre las $w_i$ (La matriz $\Gamma_i$ se denomina regularizador de Tikhonov)
    \begin{enumerate}
    \item Calcular $\Gamma$ cuando $\sum_{q=0}^Q w_q^2 \leq C$
    \item Calcular $\Gamma$ cuando $(\sum_{q=0}^Q w_q)^2 \leq C$
    \end{enumerate}
    Observar como el estudio de los regularizadores de Tikhonov puede hacerse a través de las propiedades algebraicas de las matrices $\Gamma$.
    %
    %
    \item (A) Fijar $g^-$ ( aprendida a partir de los datos de aprendizaje $\calD_{\mathrm{train}}$), y definir $\sigma^2_{\mathrm{val}} \myequ \mbox{Var}_{\calD_{\mathrm{train}}}[\Eval(g^-)]$. Veamos como $\sigma^2_{\mathrm{val}}$ depende de $K$. Sea
    
    \[
    \sigma^2(g^-)= \mbox{Var}_{\bfx}[\bfe(g^-(\bfx),y)]
    \]
    la varianza del \texttt{error-fuera-de-la-muestra} de $g^-$.
    \begin{enumerate}
        \item Mostrar que $\sigma^2_{\mathrm{val}}=\frac{1}{K}\sigma^2(g^-)$.
         \item En un problema de clasificación donde $ \bfe(g^-(\bfx),y)=\llbracket g^-(\bfx)\neq y\rrbracket$, expresar $\sigma^2_{\mathrm{val}}$ en términos de $\bbP[g^-(\bfx)\neq y]$.
         \item Mostrar que para cualquier $g^-$ en un problema de clasificación, $\sigma^2_{\mathrm{val}}=\frac{1}{4K}$.
         \item Existe una cota superior uniforme para $\mbox{Var}[\Eval(g^-)]$, similar a (c), en el caso de regresión con error cuadrático $\bfe(g^-(\bfx),y)=(g^-(\bfx)-y)^2$.
         \item En regresión con error cuadrático, si entrenamos usando pocos puntos (menos de $N-K$) para calcular $g^-$, ¿esperaría que $\sigma^2(g^-)$ sea mayor o menor? ( Ayuda: en variables aleatorias no negativas y continuas mayor valor medio muy frecuentemente implica mayor varaianza).
         \item Argumentar si aumentar el tamaño del conjunto de validación puede resultar en una mejor o peor estimación de $\Eout$. 
         \end{enumerate}
         %
         %
         \item Suponga que tenemos M modelos $\calH_1,\calH_2,\dots,\calH_M$ que entrenamos con el conjunto de entrenamiento $\calD_{\mathrm{train}}$ para obtener una hipótesis $g_m^-$ para cada modelo. Evaluamos cada una de estas hipótesis con el conjunto de evaluación para obterne sus errores de evaluación $E_1,E_2,\dots,E_M$, donde $E_m=\Eval(g_m^-)]$, para $m=1,2,\dots,M$. ¿Es $E_m$ un estimador insesgado del \texttt{error-fuera-de-la-muestra} $\Eout(g_m^-)$?
         %
         %
         \item (M) Ver la figura 4.12 del pdf( Learning from data(overfitting)).
         \begin{enumerate}
             \item $\bbE[\Eout(g^-_{m^*})]$ es inicialmente decreciente. ¿Como puede ser si $\bbE[\Eout(g^-_{m})]$ es creciente en $K$ para cada $m$? 
             \item $\bbE[\Eout(g_{m^*})]$ es inicialmente decreciente y entonces comienza a crecer. ¿Cuáles son las posibles razones para ello?
             \item Cuando $K=1$, $\bbE[\Eout(g^-_{m^*})] < \bbE[\Eout(g_{m^*})]$. ¿Cómo puede ser si las curvas de aprendizaje para ambos modelos son decrecientes ?
         \end{enumerate}
         %
         %
         \item (A) Considere el caso de minimización del error aumentado con $\mrm{\Gamma}=\rmI$ y $\mrm{\lambda=0}$:
         \begin{enumerate}
        \item Mostrar que $||\bfwa{reg}|| < ||\bfwa{lin}||$, justificando el término \texttt{``weight decay''} ( Ayuda: comenzar suponiendo $||\bfwa{reg}|| > ||\bfwa{lin}||$ y derivar una contradicción).
        %
        %
        \item Suponer que $\Ein$ es diferenciable y usar gradiente descendente para minimizar $\Eaug$:
        \[
        \bfw(t+1) \leftarrow \bfw(t)-\eta\nabla\Eaug(\bfw(t))
        \]
        Ver que esta regla de adaptación es la misma que
        \[
         \bfw(t+1) \leftarrow (1-2\lambda\eta)\bfw(t)-\eta\nabla\Ein(\bfw(t))
        \]
         Nota: esta es la razón del nombre \texttt{``weight decay''}: $\bfw(t)$ decae antes de ser adaptado por el gradiente de $\Ein$
         \end{enumerate}
         %
         %
         \item (A) Dentro de los modelos de regresión lineal se han hecho numerosos intentos de calcular el número efectivo de parámetros en un modelo. Tres de las posibilidade son
         \begin{enumerate}
         \item $d_{eff}(\mrm{\lambda})=2\mrm{trace(H(\lambda))-trace(H^2(\lambda))}$
         \item $d_{eff}(\mrm{\lambda})=\mrm{trace(H(\lambda))}$
         \item $d_{eff}(\mrm{\lambda})=\mrm{trace(H^2(\lambda))}$
         \end{enumerate}
         donde $\mrm{H(\lambda)= Z(Z^TZ+\lambda\rmI)^{-1}Z^T}$ y $\mrm{Z}$ es la matriz de datos transformados. para obtener $d_{eff}$ debemos primero calcular $\mrm{H(\lambda))}$ igual que cuando hacemos regresión. Entonce spodemos usar de forma heurística  $d_{eff}$ en lugar de $\mrm{d_{VC}}$ en la cota de generalización.
         \begin{enumerate}
             \item  Cuando $\mrm{\lambda}=0$ mostrar que para las tres elecciones, $d_{eff}=\bar{d}+1$, donde $\bar{d}$ es la dimensión en el  espacio $\calZ$.
             \item Cuando  $\mrm{\lambda}>0$ mostrar que $0\leq d_{eff}\leq\bar{d}+1$ y $d_{eff}$ es decreciente en $\mrm{\lambda}$ para las tres opciones. ( Ayuda: usar SVD)
         \end{enumerate}
         %
         %
         \item (M) Para modelos lineales y el regularizador general de Tikhonov $\mrm{\Gamma}$ con término de penalización $\mrm{\frac{\lambda}{N}\bfwt\Gamma^T\Gamma\bfw}$ en el error aumentado, mostrar que
         \[
         \mrm{\bfwa{reg}=(Z^TZ+\lambda\Gamma^T\Gamma)^{-1}Z^Ty}
         \]
         donde $\mrm{Z}$ es la matriz de características.
         \begin{enumerate}
             \item Mostrar que las predicciones dentro de la muestra son 
             \[
             \mrm{\hat{\bfy}=H(\lambda)\bfy}
             \]
             donde $\mrm{H(\lambda)=Z(Z^TZ+\lambda\Gamma^T\Gamma)^{-1}Z^T}$
             \item Simplificar esta expresión en el caso $\mrm{\Gamma=Z}$ y obtener $\bfwa{reg}$ en términos de $\bfwa{lin}$. Esto se denomina uniforme ``\texttt{weight decay}''.
         \end{enumerate}
         \item (M) (El Algoritmo LASSO). Mejor que una restricción suave sobre los cuadrados de los pesos, podriamos usar el valor absoluto de los pesos
         \[
         \mMin_{\bfw} \;\;\Ein(\mrm{\bfw}) \mbox{ suject to} \sum_{i=0}^d|\mrm{w}_i|\leq C
         \]
         el modelo se denomina el algoritmo LASSO. 
         \begin{enumerate}
             \item Formular e implementar esto como un problema de optimización cuadrática. Usar el diseño experimental enunciado en el ejercicio.1 de la sección SOBREAJUSTE de la relación.2 de prácticas para comparar el modelo \textrm{lasso} con el modelo de penalización cuadrática (también llamado ridge) dibujando el comportamiento de $\Eout$ versus el parámetro de regularización.
             \item ¿Cuál es el error aumentado? ¿Es este más conveniente para optimizar?
             \item Con $d=5$ y $N=3$ comparar los pesos obtenidos por \textrm{lasso} con los obtenidos con penalización cuadrática. ¿Qué observa como elemento más diferencial entre las soluciones obtenidas?
         \end{enumerate}
    \end{enumerate}